{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "yX_HwHNaPqNW",
        "outputId": "a15abe26-02da-475c-80f6-59f094127c46",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.0.1 requires transformers<5.0.0,>=4.34.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q torch\n",
        "!pip install -q sentence_transformers\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q huggingface-hub\n",
        "!pip install -q pypdf\n",
        "!pip install -q accelerate\n",
        "!pip install -q llama-cpp-python\n",
        "!pip install -q git+https://github.com/huggingface/transformers\n",
        "!pip install -q langchain-community\n",
        "!pip install -q torch datasets\n",
        "!pip install -q peft==0.4.0 \\\n",
        "                transformers==4.31.0 \\\n",
        "                trl==0.4.7\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vDg7-aaQVf_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gduGADZYqLH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQifbxbxQX75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "305265c5-991d-4531-e99b-fa9e013b2db8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO-fa3iGQa7G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "35caf176-e837-4743-c059-fc5d2809e6be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#load pdf files\n",
        "loader = PyPDFDirectoryLoader(\"/content/sample_data/data\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxVjkma6Qls9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ce36924b-89d7-4a8a-f3c9-d6c956c747b1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Step 05: Split the Extracted Data into Text Chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
        "\n",
        "text_chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpNEF8IfQpbm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "58f3a7dc-b3d0-4994-e531-9abf41469f21"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Step 06:Downlaod the Embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTP0cLfYQrSC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5fe3a7c7-0df6-42b0-8969-81b5b8af47d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Step 08: Create Embeddings for each of the Text Chunk\n",
        "vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z6OePGWQtMg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f188e9a2-409f-47b5-aa4d-711d02d858e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#connect to google drive\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "viblj6W9Qust",
        "outputId": "57fcc240-8768-43cc-e577-0f007f5879c6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtFZxB0FQwW0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7a7a2002-0a5e-4b1a-d195-75cb205155c6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, pipeline\n",
        "import accelerate\n",
        "\n",
        "# 1) Quantization\n",
        "from transformers import BitsAndBytesConfig\n",
        "from torch import bfloat16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i3zQ5WUQz3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7e7ca1e7-1c12-4082-d0d8-2e93f509fd4a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # 4-bit quantization\n",
        "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
        "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
        "    bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xvbqj9i3blP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cef997e5-edf9-4fb6-920b-58ae87c90867"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "# from transformers import BitsAndBytesConfig\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87BDjAKt3jdL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "17fa4107-729e-4a02-98db-c779469d159e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# # # # Load model directly\n",
        "# # # from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# # tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/Model/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",use_auth_token=\"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\")\n",
        "# # model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/Model/mistral-7b-instruct-v0.1.Q4_K_M.gguf/\",use_auth_token=\"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\")\n",
        "\n",
        "# model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "#     device_map=\"auto\",\n",
        "#     token=\"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\" # Add the authentication token here\n",
        "# )\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\n",
        "#     \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "#     token=\"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\" # Add the authentication token here\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "BSe0w1F8YXHV",
        "outputId": "0ef3d37f-7ebf-4efd-89de-6d7c19e5ec78"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/utils/utils.py:161: UserWarning: WARNING! max_new_tokens is not default parameter.\n",
            "                max_new_tokens was transferred to model_kwargs.\n",
            "                Please confirm that max_new_tokens is what you intended.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/utils/utils.py:161: UserWarning: WARNING! bits_and_bytes_config is not default parameter.\n",
            "                bits_and_bytes_config was transferred to model_kwargs.\n",
            "                Please confirm that bits_and_bytes_config is what you intended.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "llm = LlamaCpp(\n",
        "    streaming=True,\n",
        "    model_path=\"/content/drive/MyDrive/Model/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "    temperature=0.75,\n",
        "    max_new_tokens=256,\n",
        "    top_k=40,\n",
        "    top_p=1,\n",
        "    verbose=False,\n",
        "    n_ctx=4096,\n",
        "    bits_and_bytes_config=bnb_config,\n",
        "    # n_batch=1,\n",
        "    # tokenizer=tokenizer\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DASZd9bsY3LS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7f31c621-8032-4695-b864-bf4b9131d7a8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 4) Generation tuning\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "fGx8F1Q4Tei0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "298dc64c-3d24-4e2a-8aac-558697789402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "tktkB0K48UIv",
        "outputId": "e2c0b742-7c0e-4b85-eae5-4a4049041eb9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What are the qualifying conditions for CIMB personal loan product?\n",
            "Octo doc bot:  The qualifying conditions for CIMB personal loan product include the approval of CIMB in its absolute discretion and receipt by CIMB of all document(s) and information required by CIMB. CIMB also reserves the right to reject any Application in its entirety and/or approve only part of the requested Personal Loan amount at its absolute discretion without providing any reason. Furthermore, CIMB will not approve the Application if any of your account(s) are not maintained in good standing or conducted in a proper or satisfactory manner as determined by CIMB at its absolute discretion.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Query: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print('Exiting')\n",
        "        sys.exit()\n",
        "    if user_input.strip() == '':\n",
        "        continue\n",
        "\n",
        "    # Perform similarity search\n",
        "    docs = vector_store.similarity_search(user_input, k=2)\n",
        "\n",
        "    # Run the RetrievalQA chain\n",
        "    result = qa.invoke({'query': user_input, 'documents': docs})\n",
        "\n",
        "    print(f\"Octo doc bot: {result['result']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "****************"
      ],
      "metadata": {
        "id": "TkjNOcflE60v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTt5SyOJNyRc"
      },
      "outputs": [],
      "source": [
        "# 5) Pipeline\n",
        "from transformers import pipeline\n",
        "import accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myhWNHAONyZ4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the Accelerator library\n",
        "accelerator = accelerate.Accelerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLnvPUhB5KdY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm,\n",
        "    # tokenizer=tokenizer,\n",
        "    device=accelerator.device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpp-rQn_N3mF"
      },
      "outputs": [],
      "source": [
        "query = \"What are the security features of CIMB Debit Mastercard?\"\n",
        "result = generator(query, max_length=500, num_return_sequences=1, do_sample=True, top_p=0.9, top_k=50, num_beams=4)\n",
        "print(result[0][\"generated_text\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}